#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Scraper R√âEL pour jow.fr et marmiton.fr
Remplace jow_scraper_intelligent.py
"""

import requests
import json
import re
import logging
import time
import random
from typing import List, Dict, Any, Optional
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
from urllib.parse import urljoin, quote

logger = logging.getLogger(__name__)

class RealJowScraper:
    """Scraper r√©el pour jow.fr"""
    
    def __init__(self):
        self.session = requests.Session()
        self.ua = UserAgent()
        self.base_url = "https://jow.fr"
        
        # Headers r√©alistes pour √©viter la d√©tection
        self.session.headers.update({
            'User-Agent': self.ua.random,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'fr-FR,fr;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        self.delay_range = (1, 3)  # D√©lai entre requ√™tes
    
    def search_recipes(self, query: str, limit: int = 10) -> List[Dict[str, Any]]:
        """Recherche r√©elle sur jow.fr"""
        try:
            logger.info(f"üîç Recherche Jow r√©elle pour: '{query}'")
            
            # URL de recherche Jow
            search_url = f"{self.base_url}/recherche"
            params = {
                'q': query,
                'type': 'recettes'
            }
            
            # Respecter les d√©lais
            time.sleep(random.uniform(*self.delay_range))
            
            response = self.session.get(search_url, params=params, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Parser les recettes depuis le HTML
            recipes = self._parse_jow_recipes_html(soup, limit)
            
            logger.info(f"‚úÖ Trouv√© {len(recipes)} recettes Jow pour '{query}'")
            return recipes
            
        except Exception as e:
            logger.error(f"‚ùå Erreur scraping Jow: {e}")
            # Fallback vers API interne si scraping √©choue
            return self._fallback_jow_recipes(query, limit)
    
    def _parse_jow_recipes_html(self, soup: BeautifulSoup, limit: int) -> List[Dict[str, Any]]:
        """Parse les recettes depuis le HTML de Jow"""
        recipes = []
        
        # S√©lecteurs CSS pour les recettes Jow (√† ajuster selon leur structure)
        recipe_cards = soup.find_all('div', class_=['recipe-card', 'RecipeCard', 'recipe-item'])[:limit]
        
        for i, card in enumerate(recipe_cards):
            try:
                recipe = self._extract_recipe_from_card(card, f"jow_real_{i}")
                if recipe:
                    recipes.append(recipe)
            except Exception as e:
                logger.warning(f"Erreur parsing recette Jow: {e}")
                continue
        
        return recipes
    
    def _extract_recipe_from_card(self, card, recipe_id: str) -> Dict[str, Any]:
        """Extrait les donn√©es d'une recette depuis une carte HTML"""
        # Nom de la recette
        name_elem = card.find(['h2', 'h3', 'h4'], class_=['title', 'name', 'recipe-title'])
        name = name_elem.get_text(strip=True) if name_elem else "Recette sans nom"
        
        # Temps de pr√©paration
        time_elem = card.find(attrs={'data-time': True}) or card.find(text=re.compile(r'\d+\s*min'))
        prep_time = self._extract_time(time_elem) if time_elem else 30
        
        # Portions
        serving_elem = card.find(text=re.compile(r'\d+\s*pers'))
        servings = self._extract_servings(serving_elem) if serving_elem else 4
        
        # Image
        img_elem = card.find('img')
        image_url = img_elem.get('src') or img_elem.get('data-src') if img_elem else None
        
        # URL de la recette pour r√©cup√©rer les ingr√©dients
        link_elem = card.find('a')
        recipe_url = urljoin(self.base_url, link_elem.get('href')) if link_elem else None
        
        # R√©cup√©rer les ingr√©dients depuis la page de d√©tail
        ingredients = self._get_recipe_ingredients(recipe_url) if recipe_url else []
        
        return {
            'id': recipe_id,
            'name': name,
            'servings': servings,
            'prepTime': prep_time,
            'difficulty': 'Moyen',
            'image': image_url,
            'ingredients': ingredients,
            'source': 'jow',
            'url': recipe_url,
            'tags': ['jow', 'scraping']
        }
    
    def _get_recipe_ingredients(self, recipe_url: str) -> List[Dict[str, Any]]:
        """R√©cup√®re les ingr√©dients depuis la page de d√©tail"""
        try:
            time.sleep(random.uniform(*self.delay_range))
            response = self.session.get(recipe_url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Chercher les ingr√©dients dans diff√©rents formats possibles
            ingredients = []
            
            # Format 1: Liste d'ingr√©dients
            ingredient_lists = soup.find_all(['ul', 'ol'], class_=['ingredients', 'ingredient-list'])
            for ul in ingredient_lists:
                for li in ul.find_all('li'):
                    ingredient_text = li.get_text(strip=True)
                    if ingredient_text:
                        parsed = self._parse_ingredient_text(ingredient_text)
                        ingredients.append(parsed)
            
            # Format 2: Divs d'ingr√©dients
            ingredient_divs = soup.find_all('div', class_=['ingredient', 'ingredient-item'])
            for div in ingredient_divs:
                ingredient_text = div.get_text(strip=True)
                if ingredient_text:
                    parsed = self._parse_ingredient_text(ingredient_text)
                    ingredients.append(parsed)
            
            return ingredients[:20]  # Limiter √† 20 ingr√©dients max
            
        except Exception as e:
            logger.warning(f"Erreur r√©cup√©ration ingr√©dients: {e}")
            return []
    
    def _parse_ingredient_text(self, text: str) -> Dict[str, Any]:
        """Parse le texte d'un ingr√©dient pour extraire quantit√©, unit√© et nom"""
        # Patterns pour parser les ingr√©dients
        patterns = [
            r'^(\d+(?:[.,]\d+)?)\s*(g|kg|ml|l|cl|dl)\s+(.+)$',  # 500g farine
            r'^(\d+(?:[.,]\d+)?)\s+(cuill√®res?\s+√†\s+(?:soupe|caf√©)|c\.?\s*√†\s*[sc]\.?)\s+(.+)$',
            r'^(\d+(?:[.,]\d+)?)\s+(tasses?|verres?|pinc√©es?|gousses?|tranches?)\s+(.+)$',
            r'^(\d+(?:[.,]\d+)?)\s+(.+)$',  # 3 oeufs
            r'^(.+)',  # Juste le nom
        ]
        
        for pattern in patterns:
            match = re.match(pattern, text.strip(), re.IGNORECASE)
            if match:
                groups = match.groups()
                if len(groups) == 3:
                    try:
                        quantity = float(groups[0].replace(',', '.'))
                        unit = groups[1].strip()
                        name = groups[2].strip()
                        return {
                            'name': name,
                            'quantity': quantity,
                            'unit': unit,
                            'originalText': text
                        }
                    except ValueError:
                        continue
                elif len(groups) == 2:
                    try:
                        quantity = float(groups[0].replace(',', '.'))
                        name = groups[1].strip()
                        return {
                            'name': name,
                            'quantity': quantity,
                            'unit': 'unit√©',
                            'originalText': text
                        }
                    except ValueError:
                        continue
                else:
                    return {
                        'name': groups[0].strip(),
                        'quantity': 1,
                        'unit': 'unit√©',
                        'originalText': text
                    }
        
        return {
            'name': text,
            'quantity': 1,
            'unit': 'unit√©',
            'originalText': text
        }
    
    def _extract_time(self, element) -> int:
        """Extrait le temps de pr√©paration"""
        if isinstance(element, str):
            text = element
        else:
            text = element.get_text() if hasattr(element, 'get_text') else str(element)
        
        time_match = re.search(r'(\d+)', text)
        return int(time_match.group(1)) if time_match else 30
    
    def _extract_servings(self, element) -> int:
        """Extrait le nombre de portions"""
        if isinstance(element, str):
            text = element
        else:
            text = element.get_text() if hasattr(element, 'get_text') else str(element)
        
        serving_match = re.search(r'(\d+)', text)
        return int(serving_match.group(1)) if serving_match else 4
    
    def _fallback_jow_recipes(self, query: str, limit: int) -> List[Dict[str, Any]]:
        """Recettes de fallback si le scraping √©choue"""
        fallback_recipes = {
            'p√¢tes': [
                {
                    'id': 'fallback_pates_1',
                    'name': 'P√¢tes √† la carbonara',
                    'servings': 4,
                    'prepTime': 20,
                    'ingredients': [
                        {'name': 'spaghetti', 'quantity': 400, 'unit': 'g'},
                        {'name': 'lardons fum√©s', 'quantity': 200, 'unit': 'g'},
                        {'name': '≈ìufs', 'quantity': 3, 'unit': 'unit√©'},
                        {'name': 'parmesan r√¢p√©', 'quantity': 100, 'unit': 'g'}
                    ],
                    'source': 'jow_fallback'
                }
            ],
            'riz': [
                {
                    'id': 'fallback_riz_1',
                    'name': 'Riz pilaf aux l√©gumes',
                    'servings': 4,
                    'prepTime': 30,
                    'ingredients': [
                        {'name': 'riz basmati', 'quantity': 300, 'unit': 'g'},
                        {'name': 'bouillon de volaille', 'quantity': 600, 'unit': 'ml'},
                        {'name': 'carotte', 'quantity': 1, 'unit': 'unit√©'},
                        {'name': 'petits pois', 'quantity': 100, 'unit': 'g'}
                    ],
                    'source': 'jow_fallback'
                }
            ]
        }
        
        query_lower = query.lower()
        for key, recipes in fallback_recipes.items():
            if key in query_lower:
                return recipes[:limit]
        
        return []

class RealMarmitonScraper:
    """Scraper r√©el pour marmiton.org"""
    
    def __init__(self):
        self.session = requests.Session()
        self.ua = UserAgent()
        self.base_url = "https://www.marmiton.org"
        
        self.session.headers.update({
            'User-Agent': self.ua.random,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'fr-FR,fr;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
        })
        
        self.delay_range = (2, 4)  # D√©lai plus long pour Marmiton
    
    def search_recipes(self, query: str, limit: int = 10) -> List[Dict[str, Any]]:
        """Recherche r√©elle sur marmiton.org"""
        try:
            logger.info(f"üîç Recherche Marmiton r√©elle pour: '{query}'")
            
            # URL de recherche Marmiton
            search_url = f"{self.base_url}/recettes/recherche.aspx"
            params = {
                'aqt': query
            }
            
            time.sleep(random.uniform(*self.delay_range))
            
            response = self.session.get(search_url, params=params, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Parser les r√©sultats de recherche
            recipes = self._parse_marmiton_search_results(soup, limit)
            
            logger.info(f"‚úÖ Trouv√© {len(recipes)} recettes Marmiton pour '{query}'")
            return recipes
            
        except Exception as e:
            logger.error(f"‚ùå Erreur scraping Marmiton: {e}")
            return self._fallback_marmiton_recipes(query, limit)
    
    def _parse_marmiton_search_results(self, soup: BeautifulSoup, limit: int) -> List[Dict[str, Any]]:
        """Parse les r√©sultats de recherche Marmiton"""
        recipes = []
        
        # S√©lecteurs pour les recettes Marmiton
        recipe_cards = soup.find_all('div', class_=['recipe-card', 'MRTN__sc-1gofnyi-0'])[:limit]
        
        for i, card in enumerate(recipe_cards):
            try:
                recipe = self._extract_marmiton_recipe(card, f"marmiton_real_{i}")
                if recipe:
                    recipes.append(recipe)
            except Exception as e:
                logger.warning(f"Erreur parsing recette Marmiton: {e}")
                continue
        
        return recipes
    
    def _extract_marmiton_recipe(self, card, recipe_id: str) -> Dict[str, Any]:
        """Extrait une recette depuis une carte Marmiton"""
        # Nom de la recette
        name_elem = card.find(['h2', 'h3', 'a'], class_=['recipe-title', 'MRTN__sc-1gofnyi-7'])
        name = name_elem.get_text(strip=True) if name_elem else "Recette Marmiton"
        
        # Lien vers la recette
        link_elem = card.find('a')
        if link_elem:
            recipe_url = urljoin(self.base_url, link_elem.get('href'))
        else:
            return None
        
        # R√©cup√©rer les d√©tails depuis la page de la recette
        recipe_details = self._get_marmiton_recipe_details(recipe_url)
        
        return {
            'id': recipe_id,
            'name': name,
            'servings': recipe_details.get('servings', 4),
            'prepTime': recipe_details.get('prepTime', 30),
            'difficulty': recipe_details.get('difficulty', 'Moyen'),
            'image': recipe_details.get('image'),
            'ingredients': recipe_details.get('ingredients', []),
            'instructions': recipe_details.get('instructions', []),
            'source': 'marmiton',
            'url': recipe_url,
            'tags': ['marmiton', 'scraping']
        }
    
    def _get_marmiton_recipe_details(self, recipe_url: str) -> Dict[str, Any]:
        """R√©cup√®re les d√©tails d'une recette Marmiton"""
        try:
            time.sleep(random.uniform(*self.delay_range))
            response = self.session.get(recipe_url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            details = {}
            
            # Ingr√©dients
            ingredients = []
            ingredient_sections = soup.find_all(['ul', 'div'], class_=['recipe-ingredients', 'mrtn__recipe_ingredients'])
            
            for section in ingredient_sections:
                for item in section.find_all(['li', 'div']):
                    text = item.get_text(strip=True)
                    if text and len(text) > 2:
                        parsed = self._parse_marmiton_ingredient(text)
                        ingredients.append(parsed)
            
            details['ingredients'] = ingredients[:15]  # Limiter √† 15
            
            # Temps de pr√©paration
            time_elem = soup.find(attrs={'data-cooktime': True}) or soup.find(text=re.compile(r'\d+\s*min'))
            details['prepTime'] = self._extract_time(time_elem) if time_elem else 30
            
            # Portions
            serving_elem = soup.find(attrs={'data-serving': True}) or soup.find(text=re.compile(r'\d+\s*pers'))
            details['servings'] = self._extract_servings(serving_elem) if serving_elem else 4
            
            # Image
            img_elem = soup.find('img', class_=['recipe-media-image', 'mrtn__recipe_media_image'])
            details['image'] = img_elem.get('src') if img_elem else None
            
            return details
            
        except Exception as e:
            logger.warning(f"Erreur d√©tails recette Marmiton: {e}")
            return {}
    
    def _parse_marmiton_ingredient(self, text: str) -> Dict[str, Any]:
        """Parse un ingr√©dient Marmiton"""
        # Utiliser la m√™me logique que Jow
        patterns = [
            r'^(\d+(?:[.,]\d+)?)\s*(g|kg|ml|l|cl|dl)\s+(.+)$',
            r'^(\d+(?:[.,]\d+)?)\s+(cuill√®res?\s+√†\s+(?:soupe|caf√©)|c\.?\s*√†\s*[sc]\.?)\s+(.+)$',
            r'^(\d+(?:[.,]\d+)?)\s+(.+)$',
            r'^(.+)',
        ]
        
        for pattern in patterns:
            match = re.match(pattern, text.strip(), re.IGNORECASE)
            if match:
                groups = match.groups()
                if len(groups) == 3:
                    try:
                        return {
                            'name': groups[2].strip(),
                            'quantity': float(groups[0].replace(',', '.')),
                            'unit': groups[1].strip(),
                            'originalText': text
                        }
                    except ValueError:
                        continue
                elif len(groups) == 2:
                    try:
                        return {
                            'name': groups[1].strip(),
                            'quantity': float(groups[0].replace(',', '.')),
                            'unit': 'unit√©',
                            'originalText': text
                        }
                    except ValueError:
                        continue
                else:
                    return {
                        'name': groups[0].strip(),
                        'quantity': 1,
                        'unit': 'unit√©',
                        'originalText': text
                    }
        
        return {
            'name': text,
            'quantity': 1,
            'unit': 'unit√©',
            'originalText': text
        }
    
    def _extract_time(self, element) -> int:
        """Extrait le temps depuis un √©l√©ment"""
        if isinstance(element, str):
            text = element
        else:
            text = element.get_text() if hasattr(element, 'get_text') else str(element)
        
        time_match = re.search(r'(\d+)', text)
        return int(time_match.group(1)) if time_match else 30
    
    def _extract_servings(self, element) -> int:
        """Extrait les portions depuis un √©l√©ment"""
        if isinstance(element, str):
            text = element
        else:
            text = element.get_text() if hasattr(element, 'get_text') else str(element)
        
        serving_match = re.search(r'(\d+)', text)
        return int(serving_match.group(1)) if serving_match else 4
    
    def _fallback_marmiton_recipes(self, query: str, limit: int) -> List[Dict[str, Any]]:
        """Recettes de fallback Marmiton"""
        fallback_recipes = [
            {
                'id': 'marmiton_fallback_1',
                'name': f'Recette traditionnelle au {query}',
                'servings': 4,
                'prepTime': 45,
                'ingredients': [
                    {'name': query, 'quantity': 300, 'unit': 'g'},
                    {'name': 'oignon', 'quantity': 1, 'unit': 'unit√©'},
                    {'name': 'huile d\'olive', 'quantity': 2, 'unit': 'cuill√®re √† soupe'}
                ],
                'source': 'marmiton_fallback'
            }
        ]
        
        return fallback_recipes[:limit]

class UnifiedRecipeScraper:
    """Scraper unifi√© qui utilise Jow ET Marmiton"""
    
    def __init__(self):
        self.jow_scraper = RealJowScraper()
        self.marmiton_scraper = RealMarmitonScraper()
    
    def search_recipes(self, query: str, limit: int = 10) -> List[Dict[str, Any]]:
        """Recherche sur les deux sources et combine les r√©sultats"""
        all_recipes = []
        
        # R√©partir les r√©sultats : 60% Jow, 40% Marmiton
        jow_limit = max(1, int(limit * 0.6))
        marmiton_limit = max(1, limit - jow_limit)
        
        try:
            # Recherche Jow
            jow_recipes = self.jow_scraper.search_recipes(query, jow_limit)
            all_recipes.extend(jow_recipes)
            
            # Recherche Marmiton
            marmiton_recipes = self.marmiton_scraper.search_recipes(query, marmiton_limit)
            all_recipes.extend(marmiton_recipes)
            
            # M√©langer les r√©sultats pour plus de diversit√©
            import random
            random.shuffle(all_recipes)
            
            logger.info(f"‚úÖ Total: {len(all_recipes)} recettes ({len(jow_recipes)} Jow + {len(marmiton_recipes)} Marmiton)")
            return all_recipes[:limit]
            
        except Exception as e:
            logger.error(f"‚ùå Erreur scraping unifi√©: {e}")
            return []

# Instance globale pour utilisation dans app.py
unified_recipe_scraper = UnifiedRecipeScraper()
